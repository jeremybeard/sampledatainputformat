## SampleDataInputFormat

You want to create sample (dummy) data on the fly in Hadoop by specifying some simple rules? Yeah, so did I!

SampleDataInputFormat generates sample records, which are then passed to the map() calls of your MapReduce job. The key has no content, and the value contains an ASCII-1 separated string of the values.

There are three methods that each field can be generated by:

* Range: Specify a start and end value and SampleDataInputFormat will pick a random value in between (start inclusive, end exclusive). This works for the 'int', 'double' and 'date' types.

* Enum: Specify a list of values and SampleDataInputFormat will pick a random value from the list.

* UUID: SampleDataInputFormat will use the Java UUID library to generate a random 128-bit value. This could be used for unique key fields.

You can also specify a weighting for the chance that each field value will be NULL.

## Usage
### Properties
* ``sampledata.mappers`` (number of mappers to start. valid for MapReduce only)
* ``sampledata.records`` (number of records to return for the whole job)
* ``sampledata.fieldnames`` (CSV list)
* ``sampledata.fields.{fieldname}.type`` ("string" or "int" or "double" or "date")
* ``sampledata.fields.{fieldname}.date.format`` (e.g. "yyyy/MM/dd")
* ``sampledata.fields.{fieldname}.nulls.weight`` (chance that a value will be NULL, domain 0.0-1.0)
* ``sampledata.fields.{fieldname}.method`` ("range" or "enum" or "uuid")
* ``sampledata.fields.{fieldname}.range.start`` (inclusive)
* ``sampledata.fields.{fieldname}.range.end`` (exclusive)
* ``sampledata.fields.{fieldname}.enum.values`` (CSV list)

### Hive

The easiest way to use SampleDataInputFormat is through Hive. The data generation rules are specified in the Hive table DDL and each SELECT from the table will bring back a new set of sample records. Nothing is stored on HDFS! Oh, snap!

Example Hive DDL: [createtable.sql](https://github.com/jeremybeard/SampleDataInputFormat/blob/master/src/scripts/createtable.sql)

It is important when using Hive that you force the query to use MapReduce. If you run a simple ``SELECT * FROM table;`` then Hive will skip MR and just use the InputFormat to return the rows to screen. This won't be able to see all the rule properties you've entered in. So instead, if you want to do such a simple query do more like ``SELECT * FROM table WHERE 1=1;``

You can also change the parameters without recreating the table from a script or within the shell, for example

    SET sampledata.records=100000;

These changes persist only for the Hive session.

Note that due to some poor design decisions in Hive, it would require extra code to enable Hive use multiple mappers with this InputFormat. Until that is added this will only run with a single mapper in Hive.

### MapReduce

This has not been tested yet, but it should be able to work fine by passing all the parameters to the MapReduce job.