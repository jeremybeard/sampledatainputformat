## SampleDataInputFormat

You want to create sample (dummy) data on the fly in Hadoop by specifying some simple rules? Yeah, so did I!

SampleDataInputFormat generates sample records, which are then passed to the map() calls of your MapReduce job. The key has no content, and the value contains an ASCII-1 separated string of the values.

Each field can be generated by one of three methods:

* Range: Specify a start and end value and SampleDataInputFormat will pick a random value in between (start inclusive, end exclusive). This works for the 'int', 'double' and 'date' types.

* Enum: Specify a list of values and SampleDataInputFormat will pick a random value from the list.

* UUID: SampleDataInputFormat will use the Java UUID library to generate a random 128-bit value. This could be used for unique key fields.

You can also specify a weighting for the chance that each field value will be NULL.

## Usage
### Properties
The rules for SampleDataInputFormat can be passed either by MapReduce job properties or by Hive TBLPROPERTIES.

<table>
  <tr>
    <th>Property</th><th>Definition</th><th>Domain</th>
  </tr>
  <tr>
    <td></td><td></td><td></td>
  </tr>
  <tr>
    <td>``sampledata.mappers``</td><td>Number of mappers (Hive forces 1)</td><td>&gt;=1</td>
  </tr>
  <tr>
    <td>``sampledata.records``</td><td>Number of records across all mappers</td><td>&gt;1</td>
  </tr>
  <tr>
    <td>``sampledata.fieldnames``</td><td>Field names to use for later properties</td><td>Comma-separated list</td>
  </tr>
  <tr>
    <td>``sampledata.fields.{fieldname}.type``</td><td>Data type</td><td>"string", "int", "double" or "date"</td>
  </tr>
  <tr>
    <td>``sampledata.fields.{fieldname}.date.format``</td><td>Format of date strings</td><td>As per java.util.date, e.g. "yyyy/MM/dd"</td>
  </tr>
  <tr>
    <td>``sampledata.fields.{fieldname}.nulls.weight``</td><td>Chance that a value will be NULL</td><td>0.0 to 1.0</td>
  </tr>
  <tr>
    <td>``sampledata.fields.{fieldname}.method``</td><td>How to generate this field</td><td>"range" or "enum" or "uuid"</td>
  </tr>
  <tr>
    <td>``sampledata.fields.{fieldname}.range.start``</td><td>Lower-bound of range. Not valid for string.</td><td>Inclusive</td>
  </tr>
  <tr>
    <td>``sampledata.fields.{fieldname}.range.end``</td><td>Upper-bound of range. Not valid for string.</td><td>Exclusive</td>
  </tr>
  <tr>
    <td>``sampledata.fields.{fieldname}.enum.values``</td><td>List of enum values</td><td>Comma-separated list</td>
  </tr>
</table>

### Hive

The easiest way to use SampleDataInputFormat is through Hive. The data generation rules are specified in the Hive table DDL and each SELECT from the table will bring back a new set of sample records. Nothing is stored on HDFS! Oh, snap!

Example Hive DDL: [createtable.sql](https://github.com/jeremybeard/SampleDataInputFormat/blob/master/src/scripts/createtable.sql)

It is important when using Hive that you force the query to use MapReduce. If you run a simple

    SELECT * FROM table;

then Hive will skip MR and just use the InputFormat to return the rows to screen. This won't be able to see all the rule properties you've entered in. So instead, if you want to do such a simple query do more like ``SELECT * FROM table WHERE 1=1;``

You can also change the parameters without recreating the table from a script or within the shell, for example

    SET sampledata.records=100000;

These changes persist only for the Hive session.

Note that due to some poor design decisions in Hive, it would require extra code to enable Hive use multiple mappers with this InputFormat. Until that is added this will only run with a single mapper in Hive.

### MapReduce

This has not been tested yet, but it should be able to work fine by passing all the parameters to the MapReduce job.